{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"rag-datasets/rag-mini-bioasq\", \"text-corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = [x for x in ds['passages'][:10000]['passage'] if x != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from embedder import Embedder\n",
    "embedder = Embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "vectorized_passages = [embedder(x) for x in passages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_vector_db import VectorDatabase\n",
    "import numpy as np\n",
    "db = VectorDatabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6974it [00:39, 176.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for text, embedding in tqdm(zip(passages, vectorized_passages)):    \n",
    "    db.insert_vector(text, embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SmolLM2 import LLM\n",
    "\n",
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant', 'content': 'You are a helpful AI assistant.'},\n",
       " {'role': 'user', 'content': 'Can you count to 10?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm sorry for the confusion, but as a chatbot, I don't have the capability to count to 10. I'm designed to assist with general questions and provide information about various topics. I'm not equipped to assist with counting or counting to 10.\"},\n",
       " {'role': 'user', 'content': 'What did i ask?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm sorry for any confusion, but as a chatbot, I don't have the ability to ask questions or provide responses. I'm designed to assist with general questions and provide information about various topics. I'm not equipped to assist with counting or counting to 10.\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.chat_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\paulilioaica\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedder = Vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.06123998e-02,  1.38313668e-02, -2.08438132e-02,  1.63279101e-02,\n",
       "       -1.02314893e-02, -4.79842387e-02, -1.73133444e-02,  3.72874513e-02,\n",
       "        4.58873548e-02,  3.44050080e-02, -1.99597534e-02, -4.46590558e-02,\n",
       "       -1.31027959e-02,  4.28411886e-02, -5.53933121e-02, -5.89799657e-02,\n",
       "        1.33577511e-02, -4.09394503e-02, -4.66402993e-02,  3.06358524e-02,\n",
       "        3.43674459e-02,  6.01748973e-02, -5.98339178e-02,  1.76856574e-02,\n",
       "        6.31807884e-03, -1.15316808e-02, -5.60417697e-02,  2.30628736e-02,\n",
       "        3.55228893e-02, -7.31256034e-04, -4.53295372e-03,  5.71253709e-02,\n",
       "        6.49388358e-02,  2.28961241e-02,  3.90828662e-02,  1.58431251e-02,\n",
       "        7.26838782e-02,  4.77340892e-02,  8.83646961e-03,  3.84404659e-02,\n",
       "        1.78163387e-02, -9.78471786e-02,  1.98528580e-02,  2.60048099e-02,\n",
       "        4.59269946e-03,  5.50390929e-02, -4.66720164e-02,  3.55937108e-02,\n",
       "       -6.16794489e-02,  1.10782019e-03, -1.61408465e-02, -2.64057796e-02,\n",
       "       -7.94925466e-02, -8.49055350e-02,  2.61209812e-02, -3.53655225e-04,\n",
       "        3.11599392e-02, -2.97599006e-02,  7.32347593e-02,  5.35759479e-02,\n",
       "       -1.56310629e-02, -1.60845798e-02, -2.86514573e-02,  3.59529480e-02,\n",
       "        9.54189524e-02,  2.77654957e-02, -4.43648659e-02, -8.86894241e-02,\n",
       "       -1.26536898e-02, -5.75489253e-02,  2.94464233e-04,  1.19049381e-02,\n",
       "        2.71183252e-02,  4.27400731e-02,  2.99871247e-02,  1.30475229e-02,\n",
       "       -3.86321060e-02, -9.74739194e-02,  6.66968301e-02,  4.01872434e-02,\n",
       "       -9.09032300e-02, -6.05849847e-02, -1.14953173e-02,  2.38779839e-02,\n",
       "       -1.13241626e-02,  6.68726340e-02,  8.63015875e-02, -2.54630651e-02,\n",
       "       -9.88529623e-02, -6.11642702e-03, -1.18364785e-02,  5.59862964e-02,\n",
       "       -6.09397590e-02, -1.24645494e-02,  1.65631436e-02, -2.15609148e-02,\n",
       "       -2.71239672e-02, -2.01427983e-03, -5.97769134e-02,  1.04869798e-01,\n",
       "       -2.73385271e-03,  1.39009384e-02, -1.16737895e-02,  1.30327735e-02,\n",
       "       -7.93048516e-02, -7.44680017e-02,  3.51161361e-02, -4.34058756e-02,\n",
       "        6.08213022e-02,  1.67523118e-04, -5.66425771e-02, -2.63915062e-02,\n",
       "        6.10226132e-02,  5.60163371e-02,  1.75863672e-02, -2.03736126e-02,\n",
       "       -1.15453996e-01,  4.47524004e-02,  2.97224000e-02,  3.30250822e-02,\n",
       "        8.17747042e-02, -7.75716221e-03, -9.87086911e-04, -3.36253829e-02,\n",
       "        1.36572449e-02, -2.25463565e-02,  1.93044823e-02, -6.21876919e-33,\n",
       "       -1.28737139e-02, -4.17500958e-02,  2.56778896e-02,  7.84973428e-02,\n",
       "       -1.62939448e-02,  1.92020591e-02, -3.53076644e-02,  4.85241450e-02,\n",
       "       -1.03479614e-02, -2.13462226e-02, -2.83877067e-02, -7.11880848e-02,\n",
       "       -1.11056597e-03,  1.64335836e-02,  8.22415650e-02,  1.10240787e-01,\n",
       "       -1.34500917e-02,  1.06408954e-01, -7.29272589e-02,  6.12688772e-02,\n",
       "       -5.52479625e-02,  3.19749899e-02,  1.20614551e-03, -1.06262498e-01,\n",
       "       -8.82013068e-02, -5.34654707e-02, -1.21000679e-02, -6.84409461e-04,\n",
       "       -5.45270694e-03,  2.63383165e-02, -1.25151090e-02,  3.96009721e-02,\n",
       "       -7.33032674e-02,  3.81797478e-02, -2.60065254e-02, -2.20735241e-02,\n",
       "        1.27536189e-02, -1.97510961e-02, -1.14631420e-02,  1.41342115e-02,\n",
       "       -6.02153018e-02, -4.11556922e-02, -3.33048366e-02,  5.93990721e-02,\n",
       "        6.40875325e-02, -2.84697097e-02, -2.59947851e-02, -3.84407043e-02,\n",
       "        7.59447515e-02,  1.32905995e-03, -6.34449115e-03, -1.79645699e-03,\n",
       "       -2.40183598e-03, -1.28087066e-02, -4.04318497e-02,  5.02005294e-02,\n",
       "        3.89358103e-02,  1.75977182e-02, -4.38437127e-02,  9.62245688e-02,\n",
       "        3.81348431e-02,  3.11357211e-02, -7.58492351e-02,  1.78187806e-02,\n",
       "       -5.01298532e-02,  7.71979708e-03, -4.69492152e-02, -2.90283561e-02,\n",
       "        2.03475729e-02,  5.19399159e-02,  2.06304453e-02, -3.57713960e-02,\n",
       "        2.65221391e-02,  3.80805135e-02, -5.11925407e-02, -6.87222853e-02,\n",
       "       -3.54398042e-02,  8.03820118e-02, -3.77208032e-02, -1.94844080e-03,\n",
       "        3.77620757e-02, -1.02116488e-01,  4.41197120e-02, -7.54643977e-02,\n",
       "       -4.07208763e-02, -1.92215461e-02, -2.72592753e-02, -8.22235197e-02,\n",
       "        3.87672149e-03, -6.18520454e-02, -3.15740220e-02,  2.48136614e-02,\n",
       "        4.45990637e-03, -6.73047006e-02,  7.73228034e-02,  2.73967675e-33,\n",
       "       -7.12093115e-02,  9.14486572e-02, -6.04870394e-02,  9.68694836e-02,\n",
       "        8.65742415e-02, -2.06779446e-02,  1.29944697e-01,  7.91052543e-03,\n",
       "       -8.01462382e-02,  1.80234611e-01, -1.28359860e-03,  3.51411328e-02,\n",
       "        4.27592210e-02, -3.29237171e-02,  3.70712355e-02,  1.55030340e-02,\n",
       "        6.20820858e-02, -6.37758896e-02, -8.26836657e-03, -3.16683576e-02,\n",
       "       -9.21566859e-02,  1.26756221e-01,  5.25990129e-02,  6.08395748e-02,\n",
       "       -6.68150336e-02,  4.10242267e-02,  2.33066082e-02, -6.96190894e-02,\n",
       "       -1.56329130e-03, -1.85947288e-02,  1.57097969e-02,  3.73161421e-03,\n",
       "       -6.17552996e-02,  6.46869559e-03,  3.10193039e-02, -1.35252709e-02,\n",
       "        1.24141537e-01, -3.09903082e-02, -5.39066344e-02,  8.03865641e-02,\n",
       "        1.49547383e-02,  1.11181565e-01,  1.14945419e-01,  1.01298630e-01,\n",
       "       -8.28515831e-03,  1.89354084e-02,  1.87878963e-02, -9.81952623e-02,\n",
       "        2.12664939e-02,  5.55011295e-02, -6.60524592e-02, -8.62940308e-03,\n",
       "        2.66707353e-02,  6.04252219e-02, -4.29686047e-02,  1.69993415e-02,\n",
       "       -3.63522545e-02, -9.37567092e-04,  2.45387498e-02,  1.93322320e-02,\n",
       "       -8.25528577e-02,  7.35622048e-02, -3.60325724e-02,  4.25887667e-02,\n",
       "       -1.28817596e-02, -2.24338770e-02, -7.96351582e-02,  7.69076943e-02,\n",
       "        4.09279503e-02,  1.03095127e-02,  6.63880408e-02,  3.65984440e-02,\n",
       "       -1.33194879e-01, -5.81879653e-02,  6.46228194e-02, -9.32816938e-02,\n",
       "       -4.42889445e-02,  6.63895998e-03,  2.97526661e-02, -4.30267043e-02,\n",
       "       -4.98946197e-02, -7.60944933e-02,  3.99634195e-03,  3.47561426e-02,\n",
       "       -7.18526244e-02,  9.36536118e-02, -2.96941884e-02,  3.22479606e-02,\n",
       "       -4.72151227e-02,  2.68058125e-02, -1.34943228e-03, -3.35334735e-05,\n",
       "       -9.54539608e-03, -4.93623689e-02, -1.56789739e-02, -1.69308976e-08,\n",
       "       -9.99437459e-03, -4.98125404e-02, -5.80155523e-03,  1.17088696e-02,\n",
       "       -3.09662353e-02,  7.44881406e-02,  4.26415466e-02, -5.36421202e-02,\n",
       "       -5.35757579e-02,  5.13826637e-03,  9.43053663e-02,  5.92377596e-02,\n",
       "       -6.48353770e-02,  4.74533439e-02,  6.64837286e-02, -8.37892517e-02,\n",
       "        1.39587456e-02,  1.70820754e-03, -1.03098238e-02,  7.28248954e-02,\n",
       "       -9.26551595e-02,  2.93419007e-02,  3.09101995e-02,  4.00542505e-02,\n",
       "       -1.10965287e-02,  5.63484915e-02,  2.59453729e-02,  8.57609287e-02,\n",
       "        3.96481622e-03,  2.46858466e-02,  5.55655658e-02,  6.29291460e-02,\n",
       "       -4.08809707e-02, -5.91054596e-02, -2.18423009e-02,  6.20958805e-02,\n",
       "        3.34987715e-02, -3.53330486e-02,  2.67811958e-02,  1.78838372e-02,\n",
       "       -1.04897968e-01, -5.05928509e-03,  1.03227270e-03,  4.41653691e-02,\n",
       "       -5.98435178e-02, -4.77780327e-02, -7.83876181e-02, -2.04604119e-02,\n",
       "       -3.06056403e-02, -5.82466498e-02, -3.25836577e-02, -5.57630649e-03,\n",
       "        4.22337316e-02, -2.06611399e-02,  1.14280935e-02, -4.83138338e-02,\n",
       "       -3.62183265e-02, -1.07307667e-02, -8.76495838e-02,  3.68471891e-02,\n",
       "        1.12959862e-01, -1.68203972e-02,  9.41763893e-02, -4.48428318e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder(\"This is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from rag import RAG_ChatBot\n",
    "\n",
    "\n",
    "chatbot = RAG_ChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\paulilioaica\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['function'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHey how are you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulilioaica\\Desktop\\SimpleRag\\rag.py:42\u001b[0m, in \u001b[0;36mRAG_ChatBot.__call__\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, message):\n\u001b[1;32m---> 42\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\paulilioaica\\Desktop\\SimpleRag\\SmolLM2.py:47\u001b[0m, in \u001b[0;36mLLM.__call__\u001b[1;34m(self, input_text)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_state\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: input_text})\n\u001b[1;32m---> 47\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_args\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_state\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: output})\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\pipelines\\text_generation.py:258\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    254\u001b[0m     text_inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, KeyDataset) \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m    255\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m         chats \u001b[38;5;241m=\u001b[39m [Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs]  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\pipelines\\base.py:1237\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1230\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1231\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1234\u001b[0m         )\n\u001b[0;32m   1235\u001b[0m     )\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\pipelines\\base.py:1244\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1243\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1244\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1245\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\pipelines\\base.py:1144\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1143\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1144\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1145\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\pipelines\\text_generation.py:350\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\utils.py:1542\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1540\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1542\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\utils.py:1157\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[1;34m(self, model_kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[1;32m-> 1157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1160\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['function'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "chatbot(\"Hey how are you\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
